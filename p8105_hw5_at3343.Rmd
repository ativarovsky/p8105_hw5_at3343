---
title: "Homework 5"
author: "Alice Tivarovsky"
date: "11/3/2019"
output: github_document
editor_options: 
  chunk_output_type: inline
---

## Setup Code

```{r setup, include=TRUE}

library(tidyverse)
library(viridis)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_bw() + theme(legend.position = "bottom"))

set.seed(10)
```

## Problem 1

The code chunk below pulls in the iris dataset. 

```{r}

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))

iris_with_missing
```

Next, we define a funtion that replaces missing values for numeric and character variables. 

```{r}

iris_replace = function(x){
  
  if (is.numeric(x)) {
    y = mean(x, na.rm = TRUE)
    x = replace(x, is.na(x), y)
  } else if (is.character(x)) {
    x = replace(x, is.na(x), "virginica")
  }
}

```


Finally, we map the function to the dataset. 

```{r}

output = map_df(iris_with_missing, iris_replace)

output
```


## Problem 2

First, we make a dataframe comprised of the file names. 

```{r}

files = list.files(path = "./data")
files

```

Next, we write a function that reads in data from vector "files" and saves the result as a new variable in the dataframe. 

```{r}

read_num2 = function(x){
  
  str_c("./data/", x) %>% 
    read_csv() %>% 
    pivot_longer(
      week_1:week_8,
      names_to = "week",
      names_prefix = "week_",
      values_to = "value"
      ) %>% 
    mutate("patient_id" = str_remove(x, ".csv"))
}

```

Then we map the function to the files vector. 

```{r}

output = map_df(files, read_num2)

output
```

Next, we tidy the output data frame. 

```{r}
df_num2 = 
  output %>% 
  mutate(
    arm = str_replace(patient_id, "_..", ""), 
    arm = as_factor(arm)
  ) %>% 
  select(
    patient_id, arm, week, value
  )

df_num2
```

Finally, we construct a spaghetti plot of df_num2. 

```{r}
df_num2 %>% 
  ggplot(aes(x = week, y = value)) + 
  geom_line(aes(group = patient_id, color = arm)) + 
   labs(
    title = "Study Value by Week",
    x = "Week",
    y = "Value"
   )
```

In the experiemntal arm, the values generally increase week over week. In the control arm, the values generally fluctuate around the same point without any observable upwards or downwards trend. 


## Problem 3

First, we write a function that generates simulation data and fits a linear model.  

```{r}
n = 30
sdev = sqrt(50) 
beta_0 = 2


sim_slr = function(n, beta_0 = 2, beta_1 = 0) {
  
  sim_data = tibble(
    x_i1 = rnorm(n, 0, 1),
    y = beta_0 + beta_1*x_i1 + rnorm(n, 0, sdev)
  )
  
ls_fit = lm(y ~ x_i1, data = sim_data) %>% 
  broom::tidy() %>% 
  filter(term == "x_i1") %>% 
  select("estimate", "p.value") %>% 
  rename(beta_1_hat = estimate)

}

```


Next we run the simulation for beta_1 = 0: 
 
```{r}

sim_results = 
  rerun(10000, sim_slr(n, beta_0 = 2, beta_1 = 0)) %>% 
  bind_rows() 

sim_results

```


Now re-running the simulation for beta_1 values (1, 2, 3, 4, 5). 

```{r}
sim_results_2 = 
  tibble(beta_1 = c(1:6)) %>% 
  mutate(
    output_list = map(.x = beta_1, ~rerun(10000, sim_slr(n, beta_0 = 2, beta_1 = .x))),
    estimate_list = map(output_list, bind_rows)) %>% 
  select(-output_list) %>% 
  unnest(estimate_list) 

sim_results_2
```


Plot of true beta_1 vs proportion of times that the null was rejected. 

```{r}
sim_results_plot = 
  sim_results_2 %>% 
  group_by(beta_1) %>% 
  summarise(n = n(), sig = sum(p.value < 0.05), prop = sig/n) %>% 
  ggplot(aes(x = beta_1, y = prop)) +
  geom_point() +
  labs(
    title = "Effect Size vs Power", 
    x = "Effect Size", 
    y = "Power"
  ) +
  scale_x_continuous(
    breaks = c(1:6),
    labels = c("1", "2", "3", "4", "5", "6")
  )

sim_results_plot

```

Based on the plot, as the effect size increases (i.e. as beta increases), the power also increases, appraoching but not reaching the maximum value of 1.00 (100% power). 

Now, plotting the results for true beta vs calculated beta, followed by the same plot restricted to beta values that are statistically significant at alpha = 0.05. 

```{r}
sim_results_2 %>% 
  group_by(beta_1) %>% 
  summarise(n = n(), beta_hat_mean = mean(beta_1_hat)) %>% 
  ggplot(aes(x = beta_1, y = beta_hat_mean)) +
  geom_point() +
  labs(
    title = "Mean Beta Estimate vs True Beta", 
    x = "True beta1", 
    y = "Mean Estimated beta1"
  ) +
  scale_x_continuous(
    breaks = c(1:6),
    labels = c("1", "2", "3", "4", "5", "6")
  )
```

```{r}

sim_results_2 %>% 
  filter(p.value < 0.05) %>% 
  group_by(beta_1) %>% 
  summarise(n = n(), beta_hat_mean = mean(beta_1_hat)) %>% 
  ggplot(aes(x = beta_1, y = beta_hat_mean)) +
  geom_point() +
  labs(
    title = "Mean Beta Estimate vs True Beta, restricted to p values less than 0.05", 
    x = "True beta1", 
    y = "Mean of Estimated beta1"
  ) +
  scale_x_continuous(
    breaks = c(1:6),
    labels = c("1", "2", "3", "4", "5", "6")
  )
```

We observe that the first plot for unrestricted p-values is more precise as far as the calculated beta vs the true beta, meaning that the average of the estimated betas equals the true value at every point. When we restrict the betas to only those significantly different from the true beta, the results are less precise. This is because by filtering only significant results, we are eliminating values close to the true beta and keeping only values that diverge significantly from the true beta. As such, the relationship between the mean estimated beta and the true beta is not the precise one-to-one relationship seen in the unfiltered plot. 
